+++
date = "2017-04-02T17:08:52+02:00"
author = "Alan Orth"
title = "April, 2017"
tags = ["Notes"]

+++
## 2017-04-02

- Merge one change to CCAFS flagships that I had forgotten to remove last month ("MANAGING CLIMATE RISK"): https://github.com/ilri/DSpace/pull/317
- Quick proof-of-concept hack to add `dc.rights` to the input form, including some inline instructions/hints:

![dc.rights in the submission form](/cgspace-notes/2017/04/dc-rights.png)

- Remove redundant/duplicate text in the DSpace submission license
- Testing the CMYK patch on a collection with 650 items:

```
$ [dspace]/bin/dspace filter-media -f -i 10568/16498 -p "ImageMagick PDF Thumbnail" -v >& /tmp/filter-media-cmyk.txt
```

<!--more-->

## 2017-04-03

- Continue testing the CMYK patch on more communities:

```
$ [dspace]/bin/dspace filter-media -f -i 10568/1 -p "ImageMagick PDF Thumbnail" -v >> /tmp/filter-media-cmyk.txt 2>&1
```

- So far there are almost 500:

```
$ grep -c profile /tmp/filter-media-cmyk.txt
484
```

- Looking at the CG Core document again, I'll send some feedback to Peter and Abenet:
  - We use cg.contributor.crp to indicate the CRP(s) affiliated with the item
  - DSpace has dc.date.available, but this field isn't particularly meaningful other than as an automatic timestamp at the time of item accession (and is identical to dc.date.accessioned)
  - dc.relation exists in CGSpace, but isn't used—rather dc.relation.ispartofseries, which is used ~5,000 times to Series name and number within that series
- Also, I'm noticing some weird outliers in `cg.coverage.region`, need to remember to go correct these later:

```
dspace=# select text_value from metadatavalue where resource_type_id=2 and metadata_field_id=227;
```

## 2017-04-04

- The `filter-media` script has been running on more large communities and now there are many more CMYK PDFs that have been fixed:

```
$ grep -c profile /tmp/filter-media-cmyk.txt
1584
```

- Trying to find a way to get the number of items submitted by a certain user in 2016
- It's not possible in the DSpace search / module interfaces, but might be able to be derived from `dc.description.provenance`, as that field contains the name and email of the submitter/approver, ie:

```
Submitted by Francesca Giampieri (fgiampieri) on 2016-01-19T13:56:43Z^M
No. of bitstreams: 1^M
ILAC_Brief21_PMCA.pdf: 113462 bytes, checksum: 249fef468f401c066a119f5db687add0 (MD5)
```

- This SQL query returns fields that were submitted or approved by giampieri in 2016 and contain a "checksum" (ie, there was a bitstream in the submission):

```
dspace=# select * from metadatavalue where resource_type_id=2 and metadata_field_id=28 and text_value ~ '^(Submitted|Approved).*giampieri.*2016-.*checksum.*';
```

- Then this one does the same, but for fields that don't contain checksums (ie, there was no bitstream in the submission):

```
dspace=# select * from metadatavalue where resource_type_id=2 and metadata_field_id=28 and text_value ~ '^(Submitted|Approved).*giampieri.*2016-.*' and text_value !~ '^(Submitted|Approved).*giampieri.*2016-.*checksum.*';
```

- For some reason there seem to be way too many fields, for example there are 498 + 13 here, which is 511 items for just this one user.
- It looks like there can be a scenario where the user submitted AND approved it, so some records might be doubled...
- In that case it might just be better to see how many the user submitted (both _with_ and _without_ bitstreams):

```
dspace=# select * from metadatavalue where resource_type_id=2 and metadata_field_id=28 and text_value ~ '^Submitted.*giampieri.*2016-.*';
```

## 2017-04-05

- After doing a few more large communities it seems this is the final count of CMYK PDFs:

```
$ grep -c profile /tmp/filter-media-cmyk.txt
2505
```

## 2017-04-06

- After reading the [notes for DCAT April 2017](https://wiki.duraspace.org/display/cmtygp/DCAT+Meeting+April+2017) I am testing some new settings for PostgreSQL on DSpace Test:
  - `db.maxconnections` 30→70 (the default PostgreSQL config allows 100 connections, so DSpace's default of 30 is quite low)
  - `db.maxwait` 5000→10000
  - `db.maxidle` 8→20 (DSpace default is -1, unlimited, but we had set it to 8 earlier)
- I need to look at the Munin graphs after a few days to see if the load has changed
- Run system updates on DSpace Test and reboot the server
- Discussing harvesting CIFOR's DSpace via OAI
- Sisay added their OAI as a source to a new collection, but using the Simple Dublin Core method, so many fields are unqualified and duplicated
- Looking at the [documentation](https://wiki.duraspace.org/display/DSDOC5x/XMLUI+Configuration+and+Customization) it seems that we probably want to be using DSpace Intermediate Metadata

## 2017-04-10

- Adjust Linode CPU usage alerts on DSpace servers
  - CGSpace from 200 to 250%
  - DSpace Test from 100 to 150%
- Remove James from Linode access
- Look into having CIFOR use a sub prefix of 10568 like 10568.01
- Handle.net calls this ["derived prefixes"](https://www.handle.net/faq.html#4) and it seems this would work with DSpace if we wanted to go that route
- CIFOR is starting to test aligning their metadata more with CGSpace/CG core
- They shared a [test item](https://data.cifor.org/dspace/xmlui/handle/11463/947?show=full) which is using `cg.coverage.country`, `cg.subject.cifor`, `dc.subject`, and `dc.date.issued`
- Looking at their OAI I'm not sure it has updated as I don't see the new fields: https://data.cifor.org/dspace/oai/request?verb=ListRecords&resumptionToken=oai_dc///col_11463_6/900
- Maybe they need to make sure they are running the OAI cache refresh cron job, or maybe OAI doesn't export these?
- I added `cg.subject.cifor` to the metadata registry and I'm waiting for the harvester to re-harvest to see if it picks up more data now
- Another possiblity is that we could use a cross walk... but I've never done it.

## 2017-04-11

- Looking at the item from CIFOR it hasn't been updated yet, maybe they aren't running the cron job
- I emailed Usman from CIFOR to ask if he's running the cron job

## 2017-04-12

- CIFOR says they have cleaned their OAI cache and that the cron job for OAI import is enabled
- Now I see updated fields, like `dc.date.issued` but none from the CG or CIFOR namespaces
- Also, DSpace Test hasn't re-harvested this item yet, so I will wait one more day before forcing a re-harvest
- Looking at CIFOR's OAI using different metadata formats, like qualified Dublin Core and DSpace Intermediate Metadata:
  - QDC: https://data.cifor.org/dspace/oai/request?verb=ListRecords&resumptionToken=qdc///col_11463_6/900
  - DIM: https://data.cifor.org/dspace/oai/request?verb=ListRecords&resumptionToken=dim///col_11463_6/900
- Looking at one of CGSpace's items in OAI it doesn't seem that metadata fields other than those in the DC schema are exported:
  - https://cgspace.cgiar.org/handle/10568/33346?show=full
  - https://cgspace.cgiar.org/oai/request?verb=ListRecords&metadataPrefix=dim&set=col_10568_68619
- Side note: WTF, I just saw an item on CGSpace's OAI that is using `dc.cplace.country` and `dc.rplace.region`, which we stopped using in 2016 after the metadata migrations:

![stale metadata in OAI](/cgspace-notes/2017/04/cplace.png)

- The particular item is [10568/6](http://hdl.handle.net/10568/6) and, for what it's worth, the stale metadata only appears in the OAI view:
  - XMLUI: https://cgspace.cgiar.org/handle/10568/6?show=full
  - OAI: https://cgspace.cgiar.org/oai/request?verb=GetRecord&metadataPrefix=dim&identifier=oai:cgspace.cgiar.org:10568/6
- I don't see these fields anywhere in our source code or the database's metadata registry, so maybe it's just a cache issue
- I will have to check the OAI cron scripts on DSpace Test, and then run them on CGSpace
- Running `dspace oai import` and `dspace oai clean-cache` have zero effect, but this seems to rebuild the cache from scratch:

```
$ /home/dspacetest.cgiar.org/bin/dspace oai import -c
...
63900 items imported so far...
64000 items imported so far...
Total: 64056 items
Purging cached OAI responses.
OAI 2.0 manager action ended. It took 829 seconds.
```

- After reading some threads on the DSpace mailing list, I see that `clean-cache` is actually only for caching _responses_, ie to client requests in the OAI web application
- These are stored in `[dspace]/var/oai/requests/`
- The import command should theoretically catch situations like this where an item's metadata was updated, but in this case we changed the metadata schema and it doesn't seem to catch it (could be a bug!)
- Attempting a full rebuild of OAI on CGSpace:

```
$ export JAVA_OPTS="-Dfile.encoding=UTF-8 -Xmx1024m"
$ time schedtool -D -e ionice -c2 -n7 nice -n19 /home/cgspace.cgiar.org/bin/dspace oai import -c
...
58700 items imported so far...
Total: 58789 items
Purging cached OAI responses.
OAI 2.0 manager action ended. It took 1032 seconds.

real    17m20.156s
user    4m35.293s
sys     1m29.310s
```

- Now the data for 10568/6 is correct in OAI: https://cgspace.cgiar.org/oai/request?verb=GetRecord&metadataPrefix=dim&identifier=oai:cgspace.cgiar.org:10568/6
- Perhaps I need to file a bug for this, or at least ask on the DSpace Test mailing list?
- I wonder if we could use a crosswalk to convert to a format that CG Core wants, like `<date Type="Available">`

## 2017-04-13

- Checking the [CIFOR item on DSpace Test](https://dspacetest.cgiar.org/handle/11463/947?show=full), it still doesn't have the new metadata 
- The collection status shows this message from the harvester:

> Last Harvest Result: OAI server did not contain any updates on 2017-04-13 02:19:47.964

- I don't know why there were no updates detected, so I will reset and reimport the collection
- Usman has set up a custom crosswalk called `dimcg` that now shows CG and CIFOR metadata namespaces, but we can't use it because DSpace can only harvest DIM by default (from the harvesting user interface)
- Also worth noting that the REST interface exposes all fields in the item, including CG and CIFOR fields: https://data.cifor.org/dspace/rest/items/944?expand=metadata
- After re-importing the CIFOR collection it looks *very* good!
- It seems like they have done a full metadata migration with `dc.date.issued` and `cg.coverage.country` etc
- Submit pull request to upstream DSpace for the PDF thumbnail bug (DS-3516): https://github.com/DSpace/DSpace/pull/1709

## 2017-04-14

- DSpace committers reviewed my patch for DS-3516 and proposed a simpler idea involving incorrect use of `SelfRegisteredInputFormats`
- I tested the idea and it works, so I made a new patch: https://github.com/DSpace/DSpace/pull/1709
- I discovered that we can override metadata formats in OAI by creating a new "context": https://wiki.duraspace.org/display/DSDOC5x/OAI+2.0+Server
- This allows us to have, say a default "request" context and a "cgiar" context, both of which implement the DSpace Intermediate Metadata formats, but have the later use a overridden version that exposes CG metadata
- Compare the following results:
  - https://dspacetest.cgiar.org/oai/request?verb=GetRecord&metadataPrefix=dim&identifier=oai:dspacetest.cgiar.org:10568/6
  - https://dspacetest.cgiar.org/oai/cgiar?verb=GetRecord&metadataPrefix=dim&identifier=oai:dspacetest.cgiar.org:10568/6
- Reboot DSpace Test server to get new Linode kernel

## 2017-04-17

- CIFOR has now implemented a new "cgiar" context in their OAI that exposes CG fields, so I am re-harvesting that to see how it looks in the Discovery sidebars and searches
- See: https://data.cifor.org/dspace/oai/cgiar?verb=GetRecord&metadataPrefix=dim&identifier=oai:data.cifor.org:11463/947
- One thing we need to remember if we start using OAI is to enable the autostart of the harvester process (see `harvester.autoStart` in `dspace/config/modules/oai.cfg`)
- Error when running DSpace cleanup task on DSpace Test and CGSpace (on the same item), I need to look this up:

```
Error: ERROR: update or delete on table "bitstream" violates foreign key constraint "bundle_primary_bitstream_id_fkey" on table "bundle"
  Detail: Key (bitstream_id)=(435) is still referenced from table "bundle".
```

## 2017-04-18

- Helping Tsega test his new [CGSpace REST API Rails app](https://github.com/ilri/ckm-cgspace-rest-api) on DSpace Test
- Setup and run with:

```
$ git clone https://github.com/ilri/ckm-cgspace-rest-api.git
$ cd ckm-cgspace-rest-api/app
$ gem install bundler
$ bundle
$ cd ..
$ rails -s
```

- I used Ansible to create a PostgreSQL user that only has `SELECT` privileges on the tables it needs:

```
$ ansible linode02 -u aorth -b --become-user=postgres -K -m postgresql_user -a 'db=database name=username password=password priv=CONNECT/item:SELECT/metadatavalue:SELECT/metadatafieldregistry:SELECT/metadataschemaregistry:SELECT/collection:SELECT/handle:SELECT/bundle2bitstream:SELECT/bitstream:SELECT/bundle:SELECT/item2bundle:SELECT state=present
```

- Need to look into [running this via systemd](https://github.com/puma/puma/blob/master/docs/systemd.md)
- This is interesting for creating runnable commands from `bundle`:

```
$ bundle binstubs puma --path ./sbin
```

## 2017-04-19

- Usman sent another link to their OAI interface, where the country names are now capitalized: https://data.cifor.org/dspace/oai/cgiar?verb=GetRecord&metadataPrefix=dim&identifier=oai:data.cifor.org:11463/947
- Looking at the same item in XMLUI, the countries are not capitalized: https://data.cifor.org/dspace/xmlui/handle/11463/947?show=full
- So it seems he did it in the crosswalk!
- Keep working on Ansible stuff for deploying the CKM REST API
- We can use systemd's `Environment` stuff to pass the database parameters to Rails
- Abenet noticed that the "Workflow Statistics" option is missing now, but we have screenshots from a presentation in 2016 when it was there
- I filed a ticket with Atmire
- Looking at 933 CIAT records from Sisay, he's having problems creating a SAF bundle to import to DSpace Test
- I started by looking at his CSV in OpenRefine, and I see there a _bunch_ of fields with whitespace issues that I cleaned up:

```
value.replace(" ||","||").replace("|| ","||").replace(" || ","||")
```

- Also, all the filenames have spaces and URL encoded characters in them, so I decoded them from URL encoding:

```
unescape(value,"url")
```

- Then create the filename column using the following transform from URL:

```
value.split('/')[-1].replace(/#.*$/,"")
```

- The `replace` part is because some URLs have an anchor like `#page=14` which we obviously don't want on the filename
- Also, we need to only use the PDF on the item corresponding with page 1, so we don't end up with literally hundreds of duplicate PDFs
- Alternatively, I could export each page to a standalone PDF...

## 2017-04-20

- Atmire responded about the Workflow Statistics, saying that it had been disabled because many environments needed customization to be useful
- I re-enabled it with a hidden config key `workflow.stats.enabled = true` on DSpace Test and will evaluate adding it on CGSpace
- Looking at the CIAT data again, a bunch of items have metadata values ending in `||`, which might cause blank fields to be added at import time
- Cleaning them up with OpenRefine:

```
value.replace(/\|\|$/,"")
```

- Working with the CIAT data in OpenRefine to remove the filename column from all but the first item which requires a particular PDF, as there are many items pointing to the same PDF, which would cause hundreds of duplicates to be added if we included them in the SAF bundle
- I did some massaging in OpenRefine, flagging duplicates with stars and flags, then filtering and removing the filenames of those items

![Flagging and filtering duplicates in OpenRefine](/cgspace-notes/2017/04/openrefine-flagging-duplicates.png)

- Also there are loads of whitespace errors in almost every field, so I trimmed leading/trailing whitespace
- Unbelievable, there are also metadata values like:

```
COLLETOTRICHUM LINDEMUTHIANUM||                  FUSARIUM||GERMPLASM
```

- Add a description to the file names using:

```
value + "__description:" + cells["dc.type"].value
```

- Test import of 933 records:

```
$ [dspace]/bin/dspace import -a -e aorth@mjanja.ch -c 10568/87193 -s /home/aorth/src/CIAT-Books/SimpleArchiveFormat/ -m /tmp/ciat
$ wc -l /tmp/ciat
933 /tmp/ciat
```

- Run system updates on CGSpace and reboot server
- This includes switching nginx to using upstream with keepalive instead of direct `proxy_pass`
- Re-deploy CGSpace to latest `5_x-prod`, including the PABRA and RTB XMLUI themes, as well as the PDF processing and CMYK changes
- More work on Ansible infrastructure stuff for Tsega's CKM DSpace REST API
- I'm going to start re-processing all the PDF thumbnails on CGSpace, one community at a time:

```
$ export JAVA_OPTS="-Dfile.encoding=UTF-8 -Xmx1024m"
$ time schedtool -D -e ionice -c2 -n7 nice -n19 [dspace]/bin/dspace filter-media -f -v -i 10568/71249 -p "ImageMagick PDF Thumbnail" -v >& /tmp/filter-media-cmyk.txt
```

## 2017-04-22

- Someone on the dspace-tech mailing list responded with a suggestion about the foreign key violation in the `cleanup` task
- The solution is to remove the ID (ie set to NULL) from the `primary_bitstream_id` column in the `bundle` table
- After doing that and running the `cleanup` task again I find more bitstreams that are affected and end up with a long list of IDs that need to be fixed:

```
dspace=# update bundle set primary_bitstream_id=NULL where primary_bitstream_id in (435, 1136, 1132, 1220, 1236, 3002, 3255, 5322);
```

## 2017-04-24

- Two users mentioned some items they recently approved not showing up in the search / XMLUI
- I looked at the logs from yesterday and it seems the Discovery indexing has been crashing:

```
2017-04-24 00:00:15,578 INFO  com.atmire.dspace.discovery.AtmireSolrService @ Processing (55 of 58853): 70590
2017-04-24 00:00:15,586 INFO  com.atmire.dspace.discovery.AtmireSolrService @ Processing (56 of 58853): 74507
2017-04-24 00:00:15,614 ERROR com.atmire.dspace.discovery.AtmireSolrService @ this IndexWriter is closed
org.apache.solr.client.solrj.impl.HttpSolrServer$RemoteSolrException: this IndexWriter is closed
        at org.apache.solr.client.solrj.impl.HttpSolrServer.executeMethod(HttpSolrServer.java:552)
        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:210)
        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:206)
        at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:124)
        at org.apache.solr.client.solrj.SolrServer.deleteByQuery(SolrServer.java:285)
        at org.apache.solr.client.solrj.SolrServer.deleteByQuery(SolrServer.java:271)
        at org.dspace.discovery.SolrServiceImpl.unIndexContent(SolrServiceImpl.java:331)
        at org.dspace.discovery.SolrServiceImpl.unIndexContent(SolrServiceImpl.java:315)
        at com.atmire.dspace.discovery.AtmireSolrService.indexContent(AtmireSolrService.java:803)
        at com.atmire.dspace.discovery.AtmireSolrService.updateIndex(AtmireSolrService.java:876)
        at org.dspace.discovery.IndexClient.main(IndexClient.java:127)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.dspace.app.launcher.ScriptLauncher.runOneCommand(ScriptLauncher.java:226)
        at org.dspace.app.launcher.ScriptLauncher.main(ScriptLauncher.java:78)
```

- Looking at the past few days of logs, it looks like the indexing process started crashing on 2017-04-20:

```
# grep -c 'IndexWriter is closed' [dspace]/log/dspace.log.2017-04-*
[dspace]/log/dspace.log.2017-04-01:0
[dspace]/log/dspace.log.2017-04-02:0
[dspace]/log/dspace.log.2017-04-03:0
[dspace]/log/dspace.log.2017-04-04:0
[dspace]/log/dspace.log.2017-04-05:0
[dspace]/log/dspace.log.2017-04-06:0
[dspace]/log/dspace.log.2017-04-07:0
[dspace]/log/dspace.log.2017-04-08:0
[dspace]/log/dspace.log.2017-04-09:0
[dspace]/log/dspace.log.2017-04-10:0
[dspace]/log/dspace.log.2017-04-11:0
[dspace]/log/dspace.log.2017-04-12:0
[dspace]/log/dspace.log.2017-04-13:0
[dspace]/log/dspace.log.2017-04-14:0
[dspace]/log/dspace.log.2017-04-15:0
[dspace]/log/dspace.log.2017-04-16:0
[dspace]/log/dspace.log.2017-04-17:0
[dspace]/log/dspace.log.2017-04-18:0
[dspace]/log/dspace.log.2017-04-19:0
[dspace]/log/dspace.log.2017-04-20:2293
[dspace]/log/dspace.log.2017-04-21:5992
[dspace]/log/dspace.log.2017-04-22:13278
[dspace]/log/dspace.log.2017-04-23:22720
[dspace]/log/dspace.log.2017-04-24:21422
```

- I restarted Tomcat and re-ran the discovery process manually:

```
[dspace]/bin/dspace index-discovery
```

- Now everything is ok
- Finally finished manually running the cleanup task over and over and null'ing the conflicting IDs:

```
dspace=# update bundle set primary_bitstream_id=NULL where primary_bitstream_id in (435, 1132, 1136, 1220, 1236, 3002, 3255, 5322, 5098, 5982, 5897, 6245, 6184, 4927, 6070, 4925, 6888, 7368, 7136, 7294, 7698, 7864, 10799, 10839, 11765, 13241, 13634, 13642, 14127, 14146, 15582, 16116, 16254, 17136, 17486, 17824, 18098, 22091, 22149, 22206, 22449, 22548, 22559, 22454, 22253, 22553, 22897, 22941, 30262, 33657, 39796, 46943, 56561, 58237, 58739, 58734, 62020, 62535, 64149, 64672, 66988, 66919, 76005, 79780, 78545, 81078, 83620, 84492, 92513, 93915);
```

- Now running the cleanup script on DSpace Test and already seeing 11GB freed from the assetstore—it's likely we haven't had a cleanup task complete successfully in years...
